---
title: 'Results across platforms'
author: "Selene Fregosi"
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin = 1cm
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  comment = "",
  prompt = FALSE,
  fig.width = 10,
  fig.height = 10
)
```

Created following https://taikisan21.github.io/PAMpal/banterGuide.html

## Setup

```{r libraries, eval=T, message=FALSE}

library(banter)
library(rfPermute)
library(tidyverse)

```

```{r set_paths}

path_models <- file.path('T:/fregosi/cross_platform_banter_big_data/models')

```

## Load models and data

```{r load-models}

# towed array model
modelFile <- file.path(path_models,  'bantMDL_HICEAS2017-CH5_Filtered_577_2024-05-20.rdata')

load(modelFile)
bant.mdl_ta <- bant.mdl

bant.rf_ta <- getBanterModel(bant.mdl_ta)
bantData.df_ta <- getBanterModelData(bant.mdl_ta)

# longline model
modelFile <- file.path(path_models, 'bantMDL_LLHARP_2024-05-27_ss15_nt10k.rda')

load(modelFile)
bant.mdl_ll <- bant.mdl

bant.rf_ll <- getBanterModel(bant.mdl_ll)
bantData.df_ll <- getBanterModelData(bant.mdl_ll)

# glider model
modelFile <- file.path(path_models, 'bantMDL_glider_2024-05-28_ss5_nt12k.rda')

load(modelFile)
bant.mdl_gl <- bant.mdl

bant.rf_gl <- getBanterModel(bant.mdl_gl)
bantData.df_gl <- getBanterModelData(bant.mdl_gl)

# combined model
modelFile <- file.path(path_models, 'bantMDL_combined_2024-05-29_ss5_nt10k.rda')

load(modelFile)
bant.mdl_cp <- bant.mdl

bant.rf_cp <- getBanterModel(bant.mdl_cp)
bantData.df_cp <- getBanterModelData(bant.mdl_cp)



```

```{r load-data}

# towed array data
trainingFile <- file.path('R:/McCullough/PAMpal/Banter_TowedArray_Data',
                          'banterAllsmall_HICEAS2017-CH5_Filtered-577.rdata')

load(trainingFile) # called banterAllsmall

# longline data
trainingFile <- file.path('T:/fregosi/cross_platform_banter_big_data', 
                          'banterDetsTrain_LLHARP_2024-05-27.rda')
load(trainingFile) # called banterDetsTrain
banterDetsTrain_ll <- banterDetsTrain

# glider data
trainingFile <- file.path('T:/fregosi/cross_platform_banter_big_data',
                          'banterDetsTrain_glider_2024-05-28.rda')
load(trainingFile) # called banterDetsTrain
banterDetsTrain_gl <- banterDetsTrain

# glider data
trainingFile <- file.path('T:/fregosi/cross_platform_banter_big_data',
                          'banterDetsTrain_combined_2024-05-30.rda')
load(trainingFile) # called banterDetsTrain
banterDetsTrain_cp <- banterDetsTrain
```

## Towed array 

### Model results

#### Model Information

**Detector Names & Sample Sizes**  
Show the Detector Names and Sample Sizes
```{r model-info-ta, include=TRUE}
# Get detector names for your _BANTER_ Model
getDetectorNames(bant.mdl_ta)
# Get Sample sizes
getSampSize(bant.mdl_ta)
```

**Number of Calls & Events, Proportion of Calls**  
Number of calls (`numCalls()`), proportion of calls (`propCalls()`) and number of events (`numEvents()`) in your _BANTER_ detector models (or specify by event/species)
```{r num-calls-ta, include=TRUE}
# number of calls in detector model
numCalls(bant.mdl_ta)
# number of calls by species (can also do by event)
numCalls(bant.mdl_ta, "species")

# proportion of calls in detector model
propCalls(bant.mdl_ta)
# proportion of calls by event (can also do by species)
#propCalls(bant.mdl_ta, "event")
#[this is commented out as printout is long]

# number of events, with default for Event Model
numEvents(bant.mdl_ta)
# number of events for a specific detector 
numEvents(bant.mdl_ta, "Whistle_and_Moan_Detector")
```

#### Random Forest Summaries

The following functions are available in the `rfPermute` package and take a `randomForest` or `rfPermute` model object. Recall from above that the actual `randomForest` object can be extracted from the _BANTER_ model with the `getBanterModel()` function.

**Confusion Matrix**  
The Confusion Matrix is the most commonly used output for a Random Forest model, and is provided by `summary()`. The output includes the percent correctly classified for each species, the lower and upper confidence levels, and the priors (expected classification rate). 

By default, `summary()` reports the 95% confidence levels of the percent correctly classified. By using the `confusionMatrix()` function, we can specify a different confidence level if desired. However, unlike `summary()`, `confusionMatrix()` takes a `randomForest` object like the one we extracted above.
```{r confusion-matrix-ta, eval = FALSE, include=TRUE}
# Confusion Matrix
confusionMatrix(bant.rf_ta, conf.level = 0.75)
```
The `confusionMatrix()` function also has a `threshold` argument that provides the binomial probability that the true classification probability (given infinite data) is greater than or equal to this value. For example, if we want to know what the probability is that the true classification probability for each species is >= 0.80, we set `threshold = 0.8`:
```{r conf-mat-threshold-ta, include=TRUE}
# Confusion Matrix with medium threshold
confusionMatrix(bant.rf_ta, threshold = 0.8)
```

And alternative view of the confusion matrix comes in the form of a heat map.
```{r heat-map-ta, eval=FALSE}
# Plot Confusion Matrix Heatmap
plotConfMat(bant.rf_ta, title="Confusion Matrix HeatMap") 
```

We can also examine confusion matrices for individual detectors, such as the whistle detector ("Whistle_and_Moan_Detector"):
```{r conf-mat-wmd-ta, eval = FALSE}
wmd.rf <- getBanterModel(bant.mdl_ta, "Whistle_and_Moan_Detector")
confusionMatrix(wmd.rf)
plotConfMat(wmd.rf) 
```

**Model Percent Correct**  
This function operates on a _BANTER_ model object and provides a summary data frame with the percent of each species correctly classified for each detector model and the event model. It is a summary of the diagonal values from the confusion matrices for all models.
```{r pct-correct-models-ta, include=TRUE}
modelPctCorrect(bant.mdl_ta)
```

**Plot Votes**  
The strength of a classification model depends on the number of trees that 'voted' for the correct species. We can look at the votes from each of these 5,000 trees for an event to see how many of them were correct. This plot shows these votes where each vertical slice is an event, and the percentage of votes for each species is represented by their color. If all events for a species were to be correctly classified by all of the trees (votes) in the forest, then the plot for that species would be solid in the color that represents that species. 
```{r plot-votes-ta, include=TRUE, fig.width = 10, fig.asp = 0.6}
# Plot Vote distribution
plotVotes(bant.rf_ta) 
```

**Percent Correct**  
Another way to visualize this distribution is to evaluate the percent of events correctly classified for a given threshold (specified percent of trees in the forest voting for that species). 
```{r pct-correct-threshold-ta, include=TRUE}
# Percent Correct for a series of thresholds
pctCorrect(bant.rf_ta, pct = c(0.8, 0.6, 0.5))
```
These values will always decrease as the percent of trees threshold increases. That is because as stringency is decreased (lower thresholds), more samples are likely to be correctly classified. These values give an indication of the fraction of events that can be classified with high certainty. As we can see in this example data, the distribution goes to zero for all species at 95%. That is there are no events in any species that are correctly classified with 95% certainty.  

**Plot Predicted Probabilities**  
The full distribution of assignment probabilities to the predicted species class can be visualized with the `plotPredictedProbs()` function in `rfPermute`. Ideally, all events would be classified to the correct species (identified by the color), and would be strongly classified to the correct species (higher probablity of assignment). This plot can be used to understand the distribution of these classifications, and how strong the misclassifications were, by species.
```{r pred-prob-ta, include=TRUE, fig.width = 10, fig.asp = 0.6}
plotPredictedProbs(bant.rf_ta, bins = 30, plot = TRUE)
```

**Proximity Plot**  
The proximity plot provides a visualization of the distribution of events within the tree space. It shows the relative distance of events based on their average distance in nodes in the trees across the forest. For each event in the plot, the color of the central dot represents the true species identity, and the color of the circle represents the _BANTER_ classification. Ideally, these would form rather distinct clusters, one for each species. The wider the spread of the events in this feature space, the more variation found in these predictors. Some species differentiation may be predicted by other predictors and may not be clear based on this pair of dimensions (those may be differentiated with different predictors). 
```{r prox-plot-ta, include=TRUE, fig.width = 10}
# Proximity Plot
plotProximity(bant.rf_ta)
```

**Importance Heat Map**  
The importance heat map provides a visual assessment of the important predictors for the overall model. The _BANTER_ event model relies on the mean assignment probability for each of the detectors in our detector model, as well as any event level measures. For example, in this heat map, the first variable is  'dw.D.delphis', which is the mean probability that a detection was assigned to the species 'D.delphis' in the whistle detector. This requires extra steps to dig down to the specific whistle measures that are the important predictor variables for the whistle detector. 
```{r imp-heat-map-ta, include=TRUE}
# Importance Heat Map
plotImportance(bant.rf_ta, plot.type = "heatmap")
```

#### Mis-Classified Events

By segregating the misclassified events, you can dive deeper into these data to understand why the model failed. Perhaps they were incorrectly classified in the first place (inaccurate training data) or the misclassification could be due to natural variability in the call characteristics. There are any number of possibilities, and by diving into the misclassifications, you can learn a lot about your data and your model. We do not recommend eliminating misclassifications simply because they are misclassifications. The point is to learn more about your data, not to cherry pick your data to get the best performing model.  

**Case Predictions** 
If it is important to identify only strong classification results, they can be identified and filtered using the `casePredictions()` function in `rfPermute`.
```{r case-pred-ta, include=TRUE}
casePredict <- casePredictions(bant.rf_ta)
head(casePredict)
```

This function returns a data frame with the original and predicted species for each event along with if the event was correctly classified and the assignment probabilities to each species.  
To identify misclassified events, we just filter this data frame and grab the original event id.
```{r misclass-ta, include=TRUE}
misclass <- casePredict %>% 
  filter(!is.correct) %>%
  select(id)

casePredict[casePredict$id %in% misclass$id,]
```

We can then look closer at these events to learn more about them.

#### Variable Importance

One of the powerful features of Random Forest is the ability to assess and rank which predictors are important to the classification model. These values are based on measures of how much worse the classifier performs when the predictor variables are randomly permuted. The `importance()` function in the `randomForest` package extracts these values from a `randomForest` object.

To see the actual distribution of these values in each species, we can first identify the most important predictors (highest importance scores).

We can then plot the distribution of the predictor variables on these classes (in this case, a violin plot for each of these four most important variables). 

```{r var-imp-ta, include=TRUE}
# Get importance scores and convert to a data frame
bant.imp <- data.frame(importance(bant.rf_ta))
head(bant.imp)

# Select top 4 important event stage predictors
bant.4imp <- bant.imp[order(bant.imp$MeanDecreaseAccuracy, decreasing = TRUE), ][1:4, ]
bant.4imp

# plotImpPreds(bant.rf_ta, bantData.df_ta, "species", max.vars = 4)
```


### Predictions


```{r predict-ta}

# Check for click_detector_0 in training dataset
banterAllsmall$detectors$Click_Detector_0 <- NULL

# predict on towed array data with towed array model
# score_ta_ta <- banter::predict(bant.mdl_ta, banterAllsmall)
# save(score_ta_ta, file = file.path(path_models, 'predictions_TAbyTA.rda'))
load(file.path(path_models, 'predictions_TAbyTA.rda'))

score_ta_ta$validation.matrix

# predict on towed array data with longline model
# score_ta_ll <- banter::predict(bant.mdl_ll, banterAllsmall)
# save(score_ta_ll, file = file.path(path_models, 'predictions_TAbyLL.rda'))
load(file.path(path_models, 'predictions_TAbyLL.rda'))

score_ta_ll$validation.matrix

# predict on towed array data with glider model
# score_ta_gl <- banter::predict(bant.mdl_gl, banterAllsmall)
# save(score_ta_gl, file = file.path(path_models, 'predictions_TAbyGL.rda'))
load(file.path(path_models, 'predictions_TAbyGL.rda'))

score_ta_gl$validation.matrix
  
# predict on towed array data with combined model
score_ta_cp <- banter::predict(bant.mdl_cp, banterAllsmall)
save(score_ta_gl, file = file.path(path_models, 'predictions_TAbyCP.rda'))
load(file.path(path_models, 'predictions_TAbyCP.rda'))

score_ta_cp$validation.matrix
```


## Longline

### Model results

#### Model Information

**Detector Names & Sample Sizes**  
Show the Detector Names and Sample Sizes
```{r model-info-ll, include=TRUE}
# Get detector names for your _BANTER_ Model
getDetectorNames(bant.mdl_ll)
# Get Sample sizes
getSampSize(bant.mdl_ll)
```

**Number of Calls & Events, Proportion of Calls**  
Number of calls (`numCalls()`), proportion of calls (`propCalls()`) and number of events (`numEvents()`) in your _BANTER_ detector models (or specify by event/species)
```{r num-calls-ll, include=TRUE}
# number of calls in detector model
numCalls(bant.mdl_ll)
# number of calls by species (can also do by event)
numCalls(bant.mdl_ll, "species")

# proportion of calls in detector model
propCalls(bant.mdl_ll)
# proportion of calls by event (can also do by species)
#propCalls(bant.mdl, "event")
#[this is commented out as printout is long]

# number of events, with default for Event Model
numEvents(bant.mdl_ll)
# number of events for a specific detector 
numEvents(bant.mdl_ll, "Whistle_and_Moan_Detector")
```

#### Random Forest Summaries

The following functions are available in the `rfPermute` package and take a `randomForest` or `rfPermute` model object. Recall from above that the actual `randomForest` object can be extracted from the _BANTER_ model with the `getBanterModel()` function.

**Confusion Matrix**  
The Confusion Matrix is the most commonly used output for a Random Forest model, and is provided by `summary()`. The output includes the percent correctly classified for each species, the lower and upper confidence levels, and the priors (expected classification rate). 

By default, `summary()` reports the 95% confidence levels of the percent correctly classified. By using the `confusionMatrix()` function, we can specify a different confidence level if desired. However, unlike `summary()`, `confusionMatrix()` takes a `randomForest` object like the one we extracted above.
```{r confusion-matrix-ll, eval = FALSE, include=TRUE}
# Confusion Matrix
confusionMatrix(bant.rf_ll, conf.level = 0.75)
```

The `confusionMatrix()` function also has a `threshold` argument that provides the binomial probability that the true classification probability (given infinite data) is greater than or equal to this value. For example, if we want to know what the probability is that the true classification probability for each species is >= 0.80, we set `threshold = 0.8`:
```{r conf-mat-threshold-ll, include = TRUE}
# Confusion Matrix with medium threshold
confusionMatrix(bant.rf_ll, threshold = 0.8)
```
This shows that _D. capensis_ has a high probability of having a true classification score above 0.8 (Pr.gt_0.8 = 79.0). Conversely, the probability that the classification rate for _D.delphis_ is above 0.8 is very low (Pr.gt_0.8 = 6.8).  

And alternative view of the confusion matrix comes in the form of a heat map.
```{r heat-map-ll, eval = FALSE}
# Plot Confusion Matrix Heatmap
plotConfMat(bant.rf_ll, title="Confusion Matrix HeatMap") 
```

We can also examine confusion matrices for individual detectors, such as the whistle detector ("Whistle_and_Moan_Detector"):
```{r conf-mat-wmd-ll, eval = FALSE}
wmd.rf <- getBanterModel(bant.mdl_ll, "Whistle_and_Moan_Detector")
confusionMatrix(wmd.rf)
plotConfMat(wmd.rf) 
```

**Model Percent Correct**  
This function operates on a _BANTER_ model object and provides a summary data frame with the percent of each species correctly classified for each detector model and the event model. It is a summary of the diagonal values from the confusion matrices for all models.
```{r pct-correct-models-ll, include=TRUE}
modelPctCorrect(bant.mdl_ll)
```

**Plot Votes**  
The strength of a classification model depends on the number of trees that 'voted' for the correct species. We can look at the votes from each of these 5,000 trees for an event to see how many of them were correct. This plot shows these votes where each vertical slice is an event, and the percentage of votes for each species is represented by their color. If all events for a species were to be correctly classified by all of the trees (votes) in the forest, then the plot for that species would be solid in the color that represents that species. 
```{r plot-votes-ll, include=TRUE, fig.width = 10, fig.asp = 0.6}
# Plot Vote distribution
plotVotes(bant.rf_ll) 
```

**Percent Correct**  
Another way to visualize this distribution is to evaluate the percent of events correctly classified for a given threshold (specified percent of trees in the forest voting for that species). 
```{r pct-correct-threshold-ll, include=TRUE}
# Percent Correct for a series of thresholds
pctCorrect(bant.rf_ll, pct = c(0.8, 0.6, 0.5))
```
These values will always decrease as the percent of trees threshold increases. That is because as stringency is decreased (lower thresholds), more samples are likely to be correctly classified. These values give an indication of the fraction of events that can be classified with high certainty. As we can see in this example data, the distribution goes to zero for all species at 95%. That is there are no events in any species that are correctly classified with 95% certainty.  

**Plot Predicted Probabilities**  
The full distribution of assignment probabilities to the predicted species class can be visualized with the `plotPredictedProbs()` function in `rfPermute`. Ideally, all events would be classified to the correct species (identified by the color), and would be strongly classified to the correct species (higher probablity of assignment). This plot can be used to understand the distribution of these classifications, and how strong the misclassifications were, by species.
```{r pred-prob-ll, include=TRUE, fig.width = 10, fig.asp = 0.6}
plotPredictedProbs(bant.rf_ll, bins = 30, plot = TRUE)
```

**Proximity Plot**  
The proximity plot provides a visualization of the distribution of events within the tree space. It shows the relative distance of events based on their average distance in nodes in the trees across the forest. For each event in the plot, the color of the central dot represents the true species identity, and the color of the circle represents the _BANTER_ classification. Ideally, these would form rather distinct clusters, one for each species. The wider the spread of the events in this feature space, the more variation found in these predictors. Some species differentiation may be predicted by other predictors and may not be clear based on this pair of dimensions (those may be differentiated with different predictors). 
```{r prox-plot-ll, include=TRUE, fig.width = 10}
# Proximity Plot
plotProximity(bant.rf_ll)
```

**Importance Heat Map**  
The importance heat map provides a visual assessment of the important predictors for the overall model. The _BANTER_ event model relies on the mean assignment probability for each of the detectors in our detector model, as well as any event level measures. For example, in this heat map, the first variable is  'dw.D.delphis', which is the mean probability that a detection was assigned to the species 'D.delphis' in the whistle detector. This requires extra steps to dig down to the specific whistle measures that are the important predictor variables for the whistle detector. 
```{r imp-heat-map-ll, include=TRUE}
# Importance Heat Map
plotImportance(bant.rf_ll, plot.type = "heatmap")
```

#### Mis-Classified Events

By segregating the misclassified events, you can dive deeper into these data to understand why the model failed. Perhaps they were incorrectly classified in the first place (inaccurate training data) or the misclassification could be due to natural variability in the call characteristics. There are any number of possibilities, and by diving into the misclassifications, you can learn a lot about your data and your model. We do not recommend eliminating misclassifications simply because they are misclassifications. The point is to learn more about your data, not to cherry pick your data to get the best performing model.  

**Case Predictions** 
If it is important to identify only strong classification results, they can be identified and filtered using the `casePredictions()` function in `rfPermute`.
```{r case-pred-ll, include=TRUE}
casePredict <- casePredictions(bant.rf_ll)
head(casePredict)
```

This function returns a data frame with the original and predicted species for each event along with if the event was correctly classified and the assignment probabilities to each species.  
To identify misclassified events, we just filter this data frame and grab the original event id.
```{r misclass-ll, include=TRUE}
misclass <- casePredict %>% 
  filter(!is.correct) %>%
  select(id)

casePredict[casePredict$id %in% misclass$id,]
```

We can then look closer at these events to learn more about them.

#### Variable Importance

One of the powerful features of Random Forest is the ability to assess and rank which predictors are important to the classification model. These values are based on measures of how much worse the classifier performs when the predictor variables are randomly permuted. The `importance()` function in the `randomForest` package extracts these values from a `randomForest` object.

To see the actual distribution of these values in each species, we can first identify the most important predictors (highest importance scores).

We can then plot the distribution of the predictor variables on these classes (in this case, a violin plot for each of these four most important variables). 

```{r var-imp-ll, include=TRUE}
# Get importance scores and convert to a data frame
bant.imp <- data.frame(importance(bant.rf_ll))
head(bant.imp)

# Select top 4 important event stage predictors
bant.4imp <- bant.imp[order(bant.imp$MeanDecreaseAccuracy, decreasing = TRUE), ][1:4, ]
bant.4imp

# plotImpPreds(bant.rf_ll, bantData.df_ll, "species", max.vars = 4)
```



### Predictions

```{r predict-ll}

# have to change the species names
uSp <- unique(banterDetsTrain_ll$events$species)
uSp <- uSp[-which(uSp == 'UO' | uSp == 'Pc')]

for (u in seq_along(uSp)){
  uSpIdx <- which(banterDetsTrain_ll$events$species == uSp[u])
  banterDetsTrain_ll$events$species[uSpIdx] <- 'UO'
}

# unique(banterDetsTrain$events$species)

# Check for click_detector_0 in training dataset
banterDetsTrain_ll$detectors$Click_Detector_0 <- NULL

# predict on longline data with longline model
# score_ll_ll <- banter::predict(bant.mdl_ll, banterDetsTrain_ll)
# save(score_ll_ll, file = file.path(path_models, 'predictions_LLbyLL.rda'))
load(file.path(path_models, 'predictions_LLbyLL.rda'))
score_ll_ll$validation.matrix

# predict on longline data with towed array model
# score_ll_ta <- banter::predict(bant.mdl_ta, banterDetsTrain_ll)
# save(score_ll_ta, file = file.path(path_models, 'predictions_LLbyTA.rda'))
load(file.path(path_models, 'predictions_LLbyTA.rda'))

score_ll_ta$validation.matrix

# predict on longline data with glider model
# score_ll_gl <- banter::predict(bant.mdl_gl, banterDetsTrain_ll)
# save(score_ll_gl, file = file.path(path_models, 'predictions_LLbyGL.rda'))
load(file.path(path_models, 'predictions_LLbyGL.rda'))

score_ll_gl$validation.matrix

# predict on towed array data with combined model
score_ll_cp <- banter::predict(bant.mdl_cp, banterDetsTrain_ll)
save(score_ll_cp, file = file.path(path_models, 'predictions_LLbyCP.rda'))
load(file.path(path_models, 'predictions_LLbyCP.rda'))

score_ll_cp$validation.matrix
```

## Glider

### Model results

#### Model Information

**Detector Names & Sample Sizes**  
Show the Detector Names and Sample Sizes
```{r model-info-gl, include=TRUE}
# Get detector names for your _BANTER_ Model
getDetectorNames(bant.mdl_gl)
# Get Sample sizes
getSampSize(bant.mdl_gl)
```

**Number of Calls & Events, Proportion of Calls**  
Number of calls (`numCalls()`), proportion of calls (`propCalls()`) and number of events (`numEvents()`) in your _BANTER_ detector models (or specify by event/species)
```{r num-calls-gl, include=TRUE}
# number of calls in detector model
numCalls(bant.mdl_gl)
# number of calls by species (can also do by event)
numCalls(bant.mdl_gl, "species")

# proportion of calls in detector model
propCalls(bant.mdl_gl)
# proportion of calls by event (can also do by species)
#propCalls(bant.mdl, "event")
#[this is commented out as printout is long]

# number of events, with default for Event Model
numEvents(bant.mdl_gl)
# number of events for a specific detector 
numEvents(bant.mdl_gl, "Whistle_and_Moan_Detector")
```

#### Random Forest Summaries

The following functions are available in the `rfPermute` package and take a `randomForest` or `rfPermute` model object. Recall from above that the actual `randomForest` object can be extracted from the _BANTER_ model with the `getBanterModel()` function.

**Confusion Matrix**  
The Confusion Matrix is the most commonly used output for a Random Forest model, and is provided by `summary()`. The output includes the percent correctly classified for each species, the lower and upper confidence levels, and the priors (expected classification rate). 

By default, `summary()` reports the 95% confidence levels of the percent correctly classified. By using the `confusionMatrix()` function, we can specify a different confidence level if desired. However, unlike `summary()`, `confusionMatrix()` takes a `randomForest` object like the one we extracted above.
```{r confusion-matrix-gl, eval = FALSE, include=TRUE}
# Confusion Matrix
confusionMatrix(bant.rf_gl, conf.level = 0.75)
```

The `confusionMatrix()` function also has a `threshold` argument that provides the binomial probability that the true classification probability (given infinite data) is greater than or equal to this value. For example, if we want to know what the probability is that the true classification probability for each species is >= 0.80, we set `threshold = 0.8`:
```{r conf-mat-threshold-gl, include = TRUE}
# Confusion Matrix with medium threshold
confusionMatrix(bant.rf_gl, threshold = 0.8)
```

And alternative view of the confusion matrix comes in the form of a heat map.
```{r heat-map-gl, eval = FALSE}
# Plot Confusion Matrix Heatmap
plotConfMat(bant.rf_gl, title="Confusion Matrix HeatMap") 
```

We can also examine confusion matrices for individual detectors, such as the whistle detector ("Whistle_and_Moan_Detector"):
```{r conf-mat-wmd-gl, eval = FALSE}
wmd.rf <- getBanterModel(bant.mdl_gl, "Whistle_and_Moan_Detector")
confusionMatrix(wmd.rf)
plotConfMat(wmd.rf) 
```

**Model Percent Correct**  
This function operates on a _BANTER_ model object and provides a summary data frame with the percent of each species correctly classified for each detector model and the event model. It is a summary of the diagonal values from the confusion matrices for all models.
```{r pct-correct-models-gl, include=TRUE}
modelPctCorrect(bant.mdl_gl)
```

**Plot Votes**  
The strength of a classification model depends on the number of trees that 'voted' for the correct species. We can look at the votes from each of these 5,000 trees for an event to see how many of them were correct. This plot shows these votes where each vertical slice is an event, and the percentage of votes for each species is represented by their color. If all events for a species were to be correctly classified by all of the trees (votes) in the forest, then the plot for that species would be solid in the color that represents that species. 
```{r plot-votes-gl, include=TRUE, fig.width = 10, fig.asp = 0.6}
# Plot Vote distribution
plotVotes(bant.rf_gl) 
```

**Percent Correct**  
Another way to visualize this distribution is to evaluate the percent of events correctly classified for a given threshold (specified percent of trees in the forest voting for that species). 
```{r pct-correct-threshold-gl, include=TRUE}
# Percent Correct for a series of thresholds
pctCorrect(bant.rf_gl, pct = c(0.8, 0.6, 0.5))
```
These values will always decrease as the percent of trees threshold increases. That is because as stringency is decreased (lower thresholds), more samples are likely to be correctly classified. These values give an indication of the fraction of events that can be classified with high certainty. As we can see in this example data, the distribution goes to zero for all species at 95%. That is there are no events in any species that are correctly classified with 95% certainty.  

**Plot Predicted Probabilities**  
The full distribution of assignment probabilities to the predicted species class can be visualized with the `plotPredictedProbs()` function in `rfPermute`. Ideally, all events would be classified to the correct species (identified by the color), and would be strongly classified to the correct species (higher probablity of assignment). This plot can be used to understand the distribution of these classifications, and how strong the misclassifications were, by species.
```{r pred-prob-gl, include=TRUE, fig.width = 10, fig.asp = 0.6}
plotPredictedProbs(bant.rf_gl, bins = 30, plot = TRUE)
```

**Proximity Plot**  
The proximity plot provides a visualization of the distribution of events within the tree space. It shows the relative distance of events based on their average distance in nodes in the trees across the forest. For each event in the plot, the color of the central dot represents the true species identity, and the color of the circle represents the _BANTER_ classification. Ideally, these would form rather distinct clusters, one for each species. The wider the spread of the events in this feature space, the more variation found in these predictors. Some species differentiation may be predicted by other predictors and may not be clear based on this pair of dimensions (those may be differentiated with different predictors). 
```{r prox-plot-gl, include=TRUE, fig.width = 10}
# Proximity Plot
plotProximity(bant.rf_gl)
```

**Importance Heat Map**  
The importance heat map provides a visual assessment of the important predictors for the overall model. The _BANTER_ event model relies on the mean assignment probability for each of the detectors in our detector model, as well as any event level measures. For example, in this heat map, the first variable is  'dw.D.delphis', which is the mean probability that a detection was assigned to the species 'D.delphis' in the whistle detector. This requires extra steps to dig down to the specific whistle measures that are the important predictor variables for the whistle detector. 
```{r imp-heat-map-gl, include=TRUE}
# Importance Heat Map
plotImportance(bant.rf_gl, plot.type = "heatmap")
```

#### Mis-Classified Events

By segregating the misclassified events, you can dive deeper into these data to understand why the model failed. Perhaps they were incorrectly classified in the first place (inaccurate training data) or the misclassification could be due to natural variability in the call characteristics. There are any number of possibilities, and by diving into the misclassifications, you can learn a lot about your data and your model. We do not recommend eliminating misclassifications simply because they are misclassifications. The point is to learn more about your data, not to cherry pick your data to get the best performing model.  

**Case Predictions** 
If it is important to identify only strong classification results, they can be identified and filtered using the `casePredictions()` function in `rfPermute`.
```{r case-pred-gl, include=TRUE}
casePredict <- casePredictions(bant.rf_gl)
head(casePredict)
```

This function returns a data frame with the original and predicted species for each event along with if the event was correctly classified and the assignment probabilities to each species.  
To identify misclassified events, we just filter this data frame and grab the original event id.
```{r misclass-gl, include=TRUE}
misclass <- casePredict %>% 
  filter(!is.correct) %>%
  select(id)

casePredict[casePredict$id %in% misclass$id,]
```

We can then look closer at these events to learn more about them.

#### Variable Importance

One of the powerful features of Random Forest is the ability to assess and rank which predictors are important to the classification model. These values are based on measures of how much worse the classifier performs when the predictor variables are randomly permuted. The `importance()` function in the `randomForest` package extracts these values from a `randomForest` object.

To see the actual distribution of these values in each species, we can first identify the most important predictors (highest importance scores).

We can then plot the distribution of the predictor variables on these classes (in this case, a violin plot for each of these four most important variables). 

```{r var-imp-gl, include=TRUE}
# Get importance scores and convert to a data frame
bant.imp <- data.frame(importance(bant.rf_gl))
head(bant.imp)

# Select top 4 important event stage predictors
bant.4imp <- bant.imp[order(bant.imp$MeanDecreaseAccuracy, decreasing = TRUE), ][1:4, ]
bant.4imp

# plotImpPreds(bant.rf_ll, bantData.df_ll, "species", max.vars = 4)
```



### Predictions

```{r predict-gl}

# have to change the species names
uSp <- unique(banterDetsTrain_gl$events$species)
uSp <- uSp[-which(uSp == 'UO' | uSp == 'Pc')]

for (u in seq_along(uSp)){
  uSpIdx <- which(banterDetsTrain_gl$events$species == uSp[u])
  banterDetsTrain_gl$events$species[uSpIdx] <- 'UO'
}

# unique(banterDetsTrain$events$species)

# Check for click_detector_0 in training dataset
banterDetsTrain_gl$detectors$Click_Detector_0 <- NULL

# predict on glider data with glidermodel
# score_gl_gl <- banter::predict(bant.mdl_gl, banterDetsTrain_gl)
# save(score_gl_gl, file = file.path(path_models, 'predictions_GLbyGL.rda'))
load(file.path(path_models, 'predictions_GLbyGL.rda'))
score_gl_gl$validation.matrix

# predict on glider data with towed array
# score_gl_ta <- banter::predict(bant.mdl_ta, banterDetsTrain_gl)
# save(score_gl_ta, file = file.path(path_models, 'predictions_GLbyTA.rda'))
load(file.path(path_models, 'predictions_GLbyTA.rda'))
score_gl_ta$validation.matrix


# predict on glider data with longline
# score_gl_ll <- banter::predict(bant.mdl_ll, banterDetsTrain_gl)
# save(score_gl_ll, file = file.path(path_models, 'predictions_GLbyLL.rda'))
load(file.path(path_models, 'predictions_GLbyLL.rda'))
score_gl_ll$validation.matrix

# predict on towed array data with combined model
score_gl_cp <- banter::predict(bant.mdl_cp, banterDetsTrain_gl)
save(score_gl_cp, file = file.path(path_models, 'predictions_GLbyCP.rda'))
load(file.path(path_models, 'predictions_GLbyCP.rda'))

score_gl_cp$validation.matrix
```

## Combined

### Model results

#### Model Information

**Detector Names & Sample Sizes**  
Show the Detector Names and Sample Sizes
```{r model-info-cp, include=TRUE}
# Get detector names for your _BANTER_ Model
getDetectorNames(bant.mdl_cp)
# Get Sample sizes
getSampSize(bant.mdl_cp)
```

**Number of Calls & Events, Proportion of Calls**  
Number of calls (`numCalls()`), proportion of calls (`propCalls()`) and number of events (`numEvents()`) in your _BANTER_ detector models (or specify by event/species)
```{r num-calls-cp, include=TRUE}
# number of calls in detector model
numCalls(bant.mdl_cp)
# number of calls by species (can also do by event)
numCalls(bant.mdl_cp, "species")

# proportion of calls in detector model
propCalls(bant.mdl_cp)
# proportion of calls by event (can also do by species)
#propCalls(bant.mdl, "event")
#[this is commented out as printout is long]

# number of events, with default for Event Model
numEvents(bant.mdl_cp)
# number of events for a specific detector 
numEvents(bant.mdl_cp, "Whistle_and_Moan_Detector")
```

#### Random Forest Summaries

The following functions are available in the `rfPermute` package and take a `randomForest` or `rfPermute` model object. Recall from above that the actual `randomForest` object can be extracted from the _BANTER_ model with the `getBanterModel()` function.

**Confusion Matrix**  
The Confusion Matrix is the most commonly used output for a Random Forest model, and is provided by `summary()`. The output includes the percent correctly classified for each species, the lower and upper confidence levels, and the priors (expected classification rate). 

By default, `summary()` reports the 95% confidence levels of the percent correctly classified. By using the `confusionMatrix()` function, we can specify a different confidence level if desired. However, unlike `summary()`, `confusionMatrix()` takes a `randomForest` object like the one we extracted above.
```{r confusion-matrix-cp, eval = FALSE, include=TRUE}
# Confusion Matrix
confusionMatrix(bant.rf_cp, conf.level = 0.75)
```

The `confusionMatrix()` function also has a `threshold` argument that provides the binomial probability that the true classification probability (given infinite data) is greater than or equal to this value. For example, if we want to know what the probability is that the true classification probability for each species is >= 0.80, we set `threshold = 0.8`:
```{r conf-mat-threshold-cp, include = TRUE}
# Confusion Matrix with medium threshold
confusionMatrix(bant.rf_cp, threshold = 0.8)
```

And alternative view of the confusion matrix comes in the form of a heat map.
```{r heat-map-cp, eval = FALSE}
# Plot Confusion Matrix Heatmap
plotConfMat(bant.rf_cp, title="Confusion Matrix HeatMap") 
```

We can also examine confusion matrices for individual detectors, such as the whistle detector ("Whistle_and_Moan_Detector"):
```{r conf-mat-wmd-cp, eval = FALSE}
wmd.rf <- getBanterModel(bant.mdl_cp, "Whistle_and_Moan_Detector")
confusionMatrix(wmd.rf)
plotConfMat(wmd.rf) 
```

**Model Percent Correct**  
This function operates on a _BANTER_ model object and provides a summary data frame with the percent of each species correctly classified for each detector model and the event model. It is a summary of the diagonal values from the confusion matrices for all models.
```{r pct-correct-models-cp, include=TRUE}
modelPctCorrect(bant.mdl_cp)
```

**Plot Votes**  
The strength of a classification model depends on the number of trees that 'voted' for the correct species. We can look at the votes from each of these 5,000 trees for an event to see how many of them were correct. This plot shows these votes where each vertical slice is an event, and the percentage of votes for each species is represented by their color. If all events for a species were to be correctly classified by all of the trees (votes) in the forest, then the plot for that species would be solid in the color that represents that species. 
```{r plot-votes-cp, include=TRUE, fig.width = 10, fig.asp = 0.6}
# Plot Vote distribution
plotVotes(bant.rf_cp) 
```

**Percent Correct**  
Another way to visualize this distribution is to evaluate the percent of events correctly classified for a given threshold (specified percent of trees in the forest voting for that species). 
```{r pct-correct-threshold-cp, include=TRUE}
# Percent Correct for a series of thresholds
pctCorrect(bant.rf_cp, pct = c(0.8, 0.6, 0.5))
```
These values will always decrease as the percent of trees threshold increases. That is because as stringency is decreased (lower thresholds), more samples are likely to be correctly classified. These values give an indication of the fraction of events that can be classified with high certainty. As we can see in this example data, the distribution goes to zero for all species at 95%. That is there are no events in any species that are correctly classified with 95% certainty.  

**Plot Predicted Probabilities**  
The full distribution of assignment probabilities to the predicted species class can be visualized with the `plotPredictedProbs()` function in `rfPermute`. Ideally, all events would be classified to the correct species (identified by the color), and would be stroncpy classified to the correct species (higher probablity of assignment). This plot can be used to understand the distribution of these classifications, and how strong the misclassifications were, by species.
```{r pred-prob-cp, include=TRUE, fig.width = 10, fig.asp = 0.6}
plotPredictedProbs(bant.rf_cp, bins = 30, plot = TRUE)
```

**Proximity Plot**  
The proximity plot provides a visualization of the distribution of events within the tree space. It shows the relative distance of events based on their average distance in nodes in the trees across the forest. For each event in the plot, the color of the central dot represents the true species identity, and the color of the circle represents the _BANTER_ classification. Ideally, these would form rather distinct clusters, one for each species. The wider the spread of the events in this feature space, the more variation found in these predictors. Some species differentiation may be predicted by other predictors and may not be clear based on this pair of dimensions (those may be differentiated with different predictors). 
```{r prox-plot-cp, include=TRUE, fig.width = 10}
# Proximity Plot
plotProximity(bant.rf_cp)
```

**Importance Heat Map**  
The importance heat map provides a visual assessment of the important predictors for the overall model. The _BANTER_ event model relies on the mean assignment probability for each of the detectors in our detector model, as well as any event level measures. For example, in this heat map, the first variable is  'dw.D.delphis', which is the mean probability that a detection was assigned to the species 'D.delphis' in the whistle detector. This requires extra steps to dig down to the specific whistle measures that are the important predictor variables for the whistle detector. 
```{r imp-heat-map-cp, include=TRUE}
# Importance Heat Map
plotImportance(bant.rf_cp, plot.type = "heatmap")
```

#### Mis-Classified Events

By segregating the misclassified events, you can dive deeper into these data to understand why the model failed. Perhaps they were incorrectly classified in the first place (inaccurate training data) or the misclassification could be due to natural variability in the call characteristics. There are any number of possibilities, and by diving into the misclassifications, you can learn a lot about your data and your model. We do not recommend eliminating misclassifications simply because they are misclassifications. The point is to learn more about your data, not to cherry pick your data to get the best performing model.  

**Case Predictions** 
If it is important to identify only strong classification results, they can be identified and filtered using the `casePredictions()` function in `rfPermute`.
```{r case-pred-cp, include=TRUE}
casePredict <- casePredictions(bant.rf_cp)
head(casePredict)
```

This function returns a data frame with the original and predicted species for each event along with if the event was correctly classified and the assignment probabilities to each species.  
To identify misclassified events, we just filter this data frame and grab the original event id.
```{r misclass-cp, include=TRUE}
misclass <- casePredict %>% 
  filter(!is.correct) %>%
  select(id)

casePredict[casePredict$id %in% misclass$id,]
```

We can then look closer at these events to learn more about them.

#### Variable Importance

One of the powerful features of Random Forest is the ability to assess and rank which predictors are important to the classification model. These values are based on measures of how much worse the classifier performs when the predictor variables are randomly permuted. The `importance()` function in the `randomForest` package extracts these values from a `randomForest` object.

To see the actual distribution of these values in each species, we can first identify the most important predictors (highest importance scores).

We can then plot the distribution of the predictor variables on these classes (in this case, a violin plot for each of these four most important variables). 

```{r var-imp-cp, include=TRUE}
# Get importance scores and convert to a data frame
bant.imp <- data.frame(importance(bant.rf_cp))
head(bant.imp)

# Select top 4 important event stage predictors
bant.4imp <- bant.imp[order(bant.imp$MeanDecreaseAccuracy, decreasing = TRUE), ][1:4, ]
bant.4imp

# plotImpPreds(bant.rf_ll, bantData.df_ll, "species", max.vars = 4)
```



### Predictions

```{r predict-cp}

# have to change the species names
uSp <- unique(banterDetsTrain_cp$events$species)
uSp <- uSp[-which(uSp == 'UO' | uSp == 'Pc')]

for (u in seq_along(uSp)){
  uSpIdx <- which(banterDetsTrain_cp$events$species == uSp[u])
  banterDetsTrain_cp$events$species[uSpIdx] <- 'UO'
}

# unique(banterDetsTrain$events$species)

# Check for click_detector_0 in training dataset
banterDetsTrain_cp$detectors$Click_Detector_0 <- NULL

# predict on combined data with combined model
score_cp_cp <- banter::predict(bant.mdl_cp, banterDetsTrain_cp)
save(score_cp_cp, file = file.path(path_models, 'predictions_CPbyCP.rda'))

score_cp_cp$validation.matrix

# # predict on towed array data with combined model
# score_ta_cp <- banter::predict(bant.mdl_cp, banterAllsmall)
# save(score_ta_cp, file = file.path(path_models, 'predictions_TAbyCP.rda'))
# 
# score_ta_cp$validation.matrix
# 
# # predict on longline data with combined model
# 
# score_ll_cp <- banter::predict(bant.mdl_cp, banterDetsTrain_gl)
# save(score_ll_cp, file = file.path(path_models, 'predictions_LLbyCP.rda'))
# 
# score_ll_cp$validation.matrix
# 
# # predict on glider data with combined model
# score_gl_cp <- banter::predict(bant.mdl_cp, banterDetsTrain_cp)
# save(score_gl_cp, file = file.path(path_models, 'predictions_GLbyCP.rda'))
# 
# score_gl_cp$validation.matrix
```
